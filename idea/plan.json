[
    {
        "category": "Environment Configuration",
        "title": "Dependencies Installation, ToS Codebase Setup, and Project Structure Initialization",
        "description": "Set up the development environment for the delta-map belief update experiments on the Theory of Space (ToS) benchmark. The project is entirely API-based (0 GPU-hours): all experiments use Gemini-3 Pro via the Google AI API. The codebase builds upon the official ToS repository (release branch), which provides the spatial environment, SCOUT proxy trajectories, cognitive-map probing logic, and evaluation scripts. We modify only the cognitive-map probing prompts and the state-passing logic to implement conditions A, B, and C. The project also requires the ToS offline dataset (100 scenes with pre-rendered images and metadata) from Hugging Face.",
        "steps": {
            "step1": "**Base Environment Setup**: Create a local virtual environment in the working directory using Python's built-in venv module: `python -m venv .venv`. Activate the environment.",
            "step2": "**Clone the ToS Codebase**: Clone the official Theory-of-Space repository (release branch): `git clone --single-branch --branch release https://github.com/mll-lab-nu/Theory-of-Space.git`. This provides the spatial environment runner (`scripts/SpatialGym/spatial_run.py`), SCOUT proxy agent implementation, cognitive-map probing prompts, evaluation metrics (correctness, perception, stability, self-tracking, belief inertia), and visualization tools.",
            "step3": "**Install Dependencies**: Install hardware-agnostic Python dependencies required by the ToS codebase. Inspect the ToS repository's `requirements.txt` or `setup.py` for the dependency list (likely includes `openai`, `google-generativeai`, `Pillow`, `numpy`, `scipy`, `pandas`, `matplotlib`, `tqdm`, `pyyaml`). Install compatible versions based on the current Python version. Note: No hardware-related dependencies (PyTorch, CUDA) are needed since all experiments are API-only inference.",
            "step4": "**Download the ToS Offline Dataset**: Download the `MLL-Lab/tos-data` dataset from Hugging Face (`https://huggingface.co/datasets/MLL-Lab/tos-data`). This contains 100 scenes (run00–run99), each with `meta_data.json` (room layout, object positions, orientations), `falsebelief_exp.json` (k=4 object changes), pre-rendered images (`agent_facing_*.png`, `*_fbexp.png`, `top_down*.png`), and object/door camera views. Place the dataset under `Theory-of-Space/room_data/3-room/` (or the path expected by the ToS pipeline). We will use only scenes run00–run24 (N=25) for our experiments.",
            "step5": "**Configure Gemini-3 Pro API**: Edit `Theory-of-Space/scripts/SpatialGym/base_model_config.yaml` to add a Gemini-3 Pro entry:\n```yaml\ngemini-3-pro:\n  provider: google\n  model_name: gemini-3-pro\n  max_completion_tokens: 32768\n  temperature: 1.0\n  max_workers: 128\n  max_retries: 5\n  max_retries_api: 5\n  timeout: 500\n```\nExport the Google AI API key as an environment variable (e.g., in `setup.sh`). Validate connectivity by running a single test API call.",
            "step6": "**Create Project Extension Directory Structure**: Create directories for the experiment-specific code that extends the ToS codebase:\n```\ndelta_map_updates/\n├── prompts/               # Modified cognitive-map probing prompts for conditions A, B, C\n│   ├── condition_a.py     # Scratch regeneration prompt (original ToS style)\n│   ├── condition_b.py     # Full regen + preserve/overwrite rules prompt\n│   └── condition_c.py     # Delta-map update prompt\n├── runners/               # Modified experiment runners\n│   ├── cogmap_runner.py   # Cognitive map probing runner (initial exploration)\n│   └── falsebelief_runner.py  # False-belief revision runner\n├── evaluation/            # Extended evaluation scripts\n│   ├── cogmap_eval.py     # Cognitive map correctness evaluation\n│   ├── inertia_eval.py    # Belief inertia and identification F1 evaluation\n│   └── delta_apply.py     # Delta JSON application logic (for condition C)\n├── analysis/              # Analysis scripts\n│   ├── edit_magnitude.py  # Edit magnitude analysis\n│   └── failure_stratification.py  # Failure stratification by object type\n├── scripts/               # Shell scripts for running experiments\n├── results/               # Raw outputs and evaluation results\n│   ├── cogmap/            # Cognitive map probing results (A, B, C)\n│   └── falsebelief/       # False-belief revision results (A, B, C)\n└── configs/               # Experiment configuration files\n```"
        }
    },
    {
        "category": "Baseline Experiment",
        "title": "Condition A (Scratch Regeneration) — Cognitive Map Probing on Initial Exploration",
        "description": "Run the scratch-regeneration baseline (Condition A) for cognitive-map probing during the initial exploration phase on ToS vision world. This replicates the original ToS cognitive-map probing protocol: at each exploration step, the model receives the full observation history and generates a complete global cognitive map from scratch. This is the control condition against which Conditions B and C are compared. We use Gemini-3 Pro on N=25 scenes (run00–run24) with passive SCOUT proxy trajectories (~9–12 steps per scene). The SCOUT trajectory ensures standardized exploration coverage (360° sweep + room visitation) so that differences between conditions reflect belief-update interface quality, not exploration strategy.\n\n**Reference**: \"Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?\" | https://github.com/mll-lab-nu/Theory-of-Space",
        "steps": {
            "step1": "**Generate SCOUT Proxy Trajectories**: Use the ToS pipeline to generate passive SCOUT proxy trajectories for the N=25 scenes (run00–run24) in the vision world setting. Run the ToS `spatial_run.py` with `--exp-type passive`, `--render-mode vision`, `--num 25`, and `--model-name gemini-3-pro`. The SCOUT proxy performs a 360° sweep at each location and follows a fixed room-visitation order. Store the resulting observation sequences (images + text observations at each step) for reuse across all three conditions. Each trajectory produces a sequence of (action, observation) pairs: $\\{(a_1, O_1), (a_2, O_2), \\ldots, (a_T, O_T)\\}$ where $T \\approx 9$–$12$.",
            "step2": "**Implement Condition A Prompt and Runner**: Implement in `delta_map_updates/prompts/condition_a.py` and `delta_map_updates/runners/cogmap_runner.py`. Reference the original ToS cognitive-map probing prompt (found in the ToS codebase under `scripts/SpatialGym/` prompt templates). At each timestep $t$, the prompt provides the full observation history $\\{O_1, \\ldots, O_t\\}$ (images and text) and asks the model to output a complete global cognitive map $M_t$ in the ToS JSON format:\n```json\n{\n  \"agent\": {\"position\": [x, y], \"facing\": \"north|south|east|west\"},\n  \"obj_name\": {\"position\": [x, y], \"facing\": \"north|south|east|west\"},\n  \"gate_name\": {\"position\": [x, y], \"facing\": \"north|south|east|west\"}\n}\n```\nThe runner iterates through each scene's SCOUT trajectory, calls the Gemini-3 Pro API at each step, parses the JSON response, logs any JSON parsing failures (invalid outputs), and stores the per-step maps $\\{M_1, M_2, \\ldots, M_T\\}$ for each scene. Implement retry logic for malformed JSON responses (up to 3 retries per call) and record the retry count.",
            "step3": "**Execute Condition A on N=25 Scenes**: Run the Condition A cognitive-map probing across all 25 scenes. For each scene, iterate through the SCOUT trajectory steps, invoke the prompt at each step, and collect the resulting cognitive maps. Use Gemini-3 Pro with temperature=1.0 and max_completion_tokens=32768 (matching the ToS configuration). Store outputs in `results/cogmap/condition_a/runXX/` with per-step JSON maps and metadata (API call count, retry count, latency).",
            "step4": "**Evaluate Cognitive Map Quality**: Using the ToS evaluation pipeline, compute the following metrics on the final cognitive map $M_T$ for each scene:\n- **Final Map Correctness (overall)**: Composite of positional accuracy (pos.acc), directional accuracy (dir.acc), and facing accuracy (facing.acc), each weighted 1/3. Positional accuracy uses $(K/N) \\cdot e^{-\\text{RMSE}/L}$; directional accuracy checks pairwise direction correctness; facing accuracy checks per-object facing match.\n- **Perception** (per-turn): Local map accuracy for first-time-observed objects in the current FOV.\n- **Stability** (per-turn): Whether previously observed objects' predictions remain non-degrading.\nAlso record the JSON parsing failure rate (fraction of API calls requiring retries due to invalid JSON). Aggregate results across 25 scenes with mean and standard error."
        }
    },
    {
        "category": "Main Experiment",
        "title": "Condition B (Rule-Based Full Regeneration) — Cognitive Map Probing on Initial Exploration",
        "description": "Run the rule-based full regeneration condition (Condition B) for cognitive-map probing during initial exploration on ToS vision world. Unlike Condition A, the model receives the previous cognitive map $M_{t-1}$ as explicit context alongside the current observation $O_t$, and is instructed to output a full updated map $M_t$ following explicit preserve/overwrite rules: (1) Preserve all entries from $M_{t-1}$ unchanged unless the current observation provides evidence to change them, (2) Only update objects visible in $O_t$ or newly observed, (3) Overwrite entries that contradict the current observation. This tests whether providing the prior map and enforcing evidence-based update rules reduces belief drift caused by regeneration noise. Uses the same Gemini-3 Pro model, N=25 scenes, and SCOUT proxy trajectories generated in the Condition A experiment.",
        "steps": {
            "step1": "**Implement Condition B Prompt**: Implement in `delta_map_updates/prompts/condition_b.py`. The prompt at each timestep $t$ provides: (1) the previous cognitive map $M_{t-1}$ as verbatim JSON (for $t=1$, use an empty map `{}`), (2) the current observation $O_t$ (egocentric image + text description from the SCOUT trajectory), and (3) explicit update rules:\n- **Preserve rule**: Copy all entries from $M_{t-1}$ unchanged unless the current observation provides evidence they should change.\n- **Evidence restriction**: Only update objects that are visible in $O_t$ or newly observed.\n- **Conflict resolution**: If $O_t$ contradicts an entry for a visible object (e.g., different position or facing), overwrite that object's state to be consistent with $O_t$.\nThen ask the model to output the full updated map $M_t$ in the same ToS JSON schema. The prompt should emphasize: \"Do NOT modify entries for objects not visible in the current observation.\"",
            "step2": "**Execute Condition B on N=25 Scenes**: Reuse the SCOUT proxy trajectories generated during the Condition A experiment. For each scene, iterate through the trajectory steps. At step $t$, construct the Condition B prompt with $M_{t-1}$ (from the previous step's output) and $O_t$, call Gemini-3 Pro (temperature=1.0, max_completion_tokens=32768), parse the output JSON as $M_t$, and feed it forward to step $t+1$. Note: errors in $M_t$ propagate forward (realistic sequential update). Implement JSON validation and retry logic (up to 3 retries). If the model returns invalid JSON after retries, fall back to $M_{t-1}$ (preserve previous state). Store outputs in `results/cogmap/condition_b/runXX/` with per-step maps and metadata.",
            "step3": "**Evaluate Cognitive Map Quality**: Compute the same metrics as Condition A using the ToS evaluation pipeline:\n- **Final Map Correctness (overall)**: Composite of pos.acc, dir.acc, facing.acc (1/3 each) on $M_T$.\n- **Perception** (per-turn): Local map accuracy for newly observed objects.\n- **Stability** (per-turn): Non-degradation of previously observed objects.\n- **JSON parsing failure rate**: Fraction of API calls that required retries.\nAggregate across 25 scenes with mean and standard error."
        }
    },
    {
        "category": "Main Experiment",
        "title": "Condition C (Delta-Map Updates) — Cognitive Map Probing on Initial Exploration",
        "description": "Run the delta-map update condition (Condition C) for cognitive-map probing during initial exploration on ToS vision world. This condition uses the same inputs and update rules as Condition B, but instead of outputting a full updated map, the model outputs a compact delta JSON describing only the objects whose states should change. The evaluator then programmatically applies the delta to form $M_t = \\text{Apply}(M_{t-1}, \\delta_t)$. This tests whether reducing the model's output to only the changed entries further reduces transcription errors beyond Condition B. Uses the same Gemini-3 Pro model, N=25 scenes, and SCOUT proxy trajectories.",
        "steps": {
            "step1": "**Implement Condition C Prompt and Delta Application Logic**: Implement the prompt in `delta_map_updates/prompts/condition_c.py`. The prompt at each timestep $t$ provides: (1) the previous cognitive map $M_{t-1}$ as verbatim JSON, (2) the current observation $O_t$, and (3) the same preserve/overwrite rules as Condition B. However, the model is asked to output only a compact delta JSON:\n```json\n{\n  \"updates\": {\n    \"obj_name\": {\"position\": [x, y], \"facing\": \"east\"},\n    \"obj2\": {\"position\": [x, y]}\n  }\n}\n```\nThe `updates` object contains only the entries that should change; omitted entries are preserved from $M_{t-1}$. The prompt should instruct: \"If nothing needs to change, output {\\\"updates\\\": {}}.\" Also implement the delta application function in `delta_map_updates/evaluation/delta_apply.py`:\n```python\ndef apply_delta(M_prev: dict, delta: dict) -> dict:\n    M_new = copy.deepcopy(M_prev)\n    for key, value in delta.get(\"updates\", {}).items():\n        if key in M_new:\n            M_new[key].update(value)\n        else:\n            M_new[key] = value\n    return M_new\n```",
            "step2": "**Execute Condition C on N=25 Scenes**: Reuse the same SCOUT proxy trajectories. For each scene, iterate through trajectory steps. At step $t$, construct the Condition C prompt with $M_{t-1}$ and $O_t$, call Gemini-3 Pro (temperature=1.0, max_completion_tokens=32768), parse the delta JSON, apply it to $M_{t-1}$ to obtain $M_t$, and feed $M_t$ forward. Implement validation: check that the delta is valid JSON with an \"updates\" key, and that each update contains valid position/facing fields. Retry up to 3 times on invalid JSON; if all retries fail, use an empty delta (preserve $M_{t-1}$). Store outputs in `results/cogmap/condition_c/runXX/` with per-step deltas, applied maps, and metadata.",
            "step3": "**Evaluate Cognitive Map Quality**: Compute the same metrics as Conditions A and B:\n- **Final Map Correctness (overall)**: Composite of pos.acc, dir.acc, facing.acc (1/3 each) on $M_T$.\n- **Perception** (per-turn): Local map accuracy for newly observed objects.\n- **Stability** (per-turn): Non-degradation of previously observed objects.\n- **JSON parsing failure rate**: Fraction of API calls that required retries.\nAggregate across 25 scenes with mean and standard error. Additionally, compare output token counts between Condition C (delta output) and Conditions A/B (full map output) to quantify the reduction in generation length."
        }
    },
    {
        "category": "Baseline Experiment",
        "title": "Condition A (Scratch Regeneration) — False-Belief Revision",
        "description": "Run the scratch-regeneration baseline (Condition A) for the false-belief revision task on ToS vision world. After the initial exploration phase, k=4 objects in each scene are secretly relocated or reoriented (specified by `falsebelief_exp.json`). The agent, retaining its exploration history, must re-explore using post-change images (`*_fbexp.png`) and update its cognitive map. Under Condition A, the model regenerates the entire map from the full observation history (including both pre-change and post-change observations) at each re-exploration step. This establishes the baseline for belief inertia and change identification metrics. Uses Gemini-3 Pro on N=25 scenes with a fixed post-change SCOUT-like re-exploration trajectory.\n\n**Reference**: \"Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?\" | https://github.com/mll-lab-nu/Theory-of-Space",
        "steps": {
            "step1": "**Prepare False-Belief Re-Exploration Trajectories**: For each of the N=25 scenes (run00–run24), load `falsebelief_exp.json` to identify which k=4 objects were changed (position or orientation). Construct a fixed post-change re-exploration trajectory using a SCOUT-like strategy: the agent revisits rooms in the same order as the initial SCOUT trajectory, performing 360° sweeps, but now observes the post-change scene images (`*_fbexp.png`). This produces a new observation sequence $\\{O'_1, O'_2, \\ldots, O'_{T'}\\}$ for the re-exploration phase. The agent starts with the cognitive map $M_T$ from the end of the initial exploration (use the Condition A initial-exploration result as the starting belief).",
            "step2": "**Implement False-Belief Runner for Condition A**: Implement in `delta_map_updates/runners/falsebelief_runner.py`. At each re-exploration step $t'$, provide the full observation history (initial exploration + re-exploration up to $t'$) and ask the model to regenerate the complete cognitive map from scratch (same prompt as Condition A initial exploration, but with the extended history including post-change observations). Call Gemini-3 Pro (temperature=1.0, max_completion_tokens=32768), parse the JSON, and record the per-step maps. Store outputs in `results/falsebelief/condition_a/runXX/`.",
            "step3": "**Evaluate False-Belief Revision Metrics**: Using the ToS false-belief evaluation pipeline, compute the following metrics on the final post-revision map for each scene:\n- **Identification F1**: F1 score for detecting which of the k=4 objects changed position or orientation. Compare the predicted map changes between pre- and post-revision to the ground-truth change set.\n- **Belief Inertia (positional)**: For each changed object $i$, compute whether the post-revision error $e_i = b^{\\text{new}}_i - g^{\\text{new}}_i$ aligns with the direction of the old belief $v_i = b^{\\text{old}}_i - g^{\\text{new}}_i$, using the cosine-similarity-based inertia metric defined by ToS. Lower is better.\n- **Belief Inertia (orientation)**: Fraction of changed objects whose post-revision facing direction matches the old (pre-change) facing rather than the new ground truth. Lower is better.\n- **Belief Correctness on changed objects**: Map correctness computed only on the k=4 changed objects.\nAggregate across 25 scenes with mean and standard error."
        }
    },
    {
        "category": "Main Experiment",
        "title": "Condition B (Rule-Based Full Regeneration) — False-Belief Revision",
        "description": "Run the rule-based full regeneration condition (Condition B) for the false-belief revision task on ToS vision world. During re-exploration, the model receives the previous cognitive map $M'_{t'-1}$ (initialized as $M_T$ from Condition B's initial exploration) and the current post-change observation $O'_{t'}$, with explicit preserve/overwrite rules. This tests whether providing the prior map and conflict-resolution rules helps the model overwrite obsolete beliefs when the environment has changed. Uses the same N=25 scenes, Gemini-3 Pro model, and post-change re-exploration trajectories prepared in the Condition A false-belief experiment.",
        "steps": {
            "step1": "**Execute Condition B False-Belief Revision on N=25 Scenes**: Reuse the post-change re-exploration trajectories from the Condition A false-belief experiment. Initialize the cognitive map as $M'_0 = M_T^{(B)}$, i.e., the final map from Condition B's initial exploration phase. At each re-exploration step $t'$, construct the Condition B prompt with $M'_{t'-1}$ and the post-change observation $O'_{t'}$ (using the same preserve/overwrite rules as in the initial exploration). Call Gemini-3 Pro (temperature=1.0, max_completion_tokens=32768), parse the full updated map, and propagate forward. Store outputs in `results/falsebelief/condition_b/runXX/`.",
            "step2": "**Evaluate False-Belief Revision Metrics**: Compute the same false-belief evaluation metrics as Condition A:\n- **Identification F1** for detecting changed objects.\n- **Belief Inertia (positional and orientation)** — lower is better.\n- **Belief Correctness on changed objects**.\n- **JSON parsing failure rate** during re-exploration.\nAggregate across 25 scenes with mean and standard error."
        }
    },
    {
        "category": "Main Experiment",
        "title": "Condition C (Delta-Map Updates) — False-Belief Revision",
        "description": "Run the delta-map update condition (Condition C) for the false-belief revision task on ToS vision world. During re-exploration, the model receives the previous cognitive map $M'_{t'-1}$ (initialized as $M_T$ from Condition C's initial exploration) and the current post-change observation, and outputs only a delta JSON with changed entries. The delta is applied programmatically. This tests whether the compact delta format improves belief revision (overwriting obsolete entries) compared to full regeneration. Uses the same N=25 scenes, Gemini-3 Pro model, and post-change re-exploration trajectories.",
        "steps": {
            "step1": "**Execute Condition C False-Belief Revision on N=25 Scenes**: Reuse the post-change re-exploration trajectories. Initialize the cognitive map as $M'_0 = M_T^{(C)}$, i.e., the final map from Condition C's initial exploration phase. At each re-exploration step $t'$, construct the Condition C prompt with $M'_{t'-1}$ and the post-change observation $O'_{t'}$ (same delta-output instructions as in initial exploration). Call Gemini-3 Pro (temperature=1.0, max_completion_tokens=32768), parse the delta JSON, apply it to $M'_{t'-1}$ using `apply_delta()`, and propagate the resulting $M'_{t'}$ forward. Store outputs in `results/falsebelief/condition_c/runXX/`.",
            "step2": "**Evaluate False-Belief Revision Metrics**: Compute the same false-belief evaluation metrics:\n- **Identification F1** for detecting changed objects.\n- **Belief Inertia (positional and orientation)** — lower is better.\n- **Belief Correctness on changed objects**.\n- **JSON parsing failure rate** during re-exploration.\nAggregate across 25 scenes with mean and standard error. Additionally, compare the output token counts between Condition C and Conditions A/B during the re-exploration phase."
        }
    },
    {
        "category": "Effectiveness Evaluation",
        "title": "Effectiveness Evaluation of Delta-Map Belief Updates",
        "description": "Evaluate whether the proposed belief-update interface (Conditions B and C) is effective at improving cognitive-map quality and reducing false-belief inertia compared to the scratch-regeneration baseline (Condition A). The evaluation covers two success criteria: (1) Condition B reduces belief inertia and improves identification F1 vs Condition A on the false-belief revision task, validated by paired bootstrap confidence intervals; (2) Condition C matches or improves over B while producing fewer invalid JSON outputs. The assessment considers both the cognitive map probing (initial exploration) and the false-belief revision task results.",
        "steps": {
            "step1": "**Collect All Experimental Results**: Gather the following results from the preceding experiments:\n- **Cognitive Map Probing (Initial Exploration)**: For each condition (A, B, C), collect final map correctness (overall, pos.acc, dir.acc, facing.acc), perception, stability, and JSON parsing failure rate across N=25 scenes.\n- **False-Belief Revision**: For each condition (A, B, C), collect identification F1, belief inertia (positional and orientation), belief correctness on changed objects, and JSON parsing failure rate across N=25 scenes.\nAlso note the reference numbers from the ToS paper for Gemini-3 Pro on the full dataset (N=100): final correctness 52.1%, positional inertia 51.1%, orientation inertia 14.4%. These are context-only (not directly comparable to the N=25 subset).",
            "step2": "**Evaluate Success Criterion 1 (B improves over A on False-Belief Revision)**: For the false-belief revision task, compare Condition B vs Condition A on:\n- **Identification F1**: Compute paired difference (B − A) across 25 scenes. Compute 95% paired bootstrap confidence interval (10,000 resamples) for the mean difference. The criterion is met if the CI excludes 0 in the direction of B > A.\n- **Belief Inertia (positional)**: Compute paired difference (A − B, since lower is better). Compute 95% paired bootstrap CI. The criterion is met if the CI excludes 0 in the direction of A > B.\n- **Belief Inertia (orientation)**: Same paired bootstrap test.\nReport whether Criterion 1 passes or fails for each sub-metric.",
            "step3": "**Evaluate Success Criterion 2 (C matches/improves over B with fewer errors)**: Compare Condition C vs Condition B:\n- **Correctness/Inertia**: On both cognitive map probing and false-belief revision, check whether C achieves comparable or better scores than B. Use paired bootstrap CIs to assess whether C ≥ B (non-inferiority).\n- **JSON parsing failure rate**: Compare the retry rates between C and B across both tasks. A lower retry rate for C supports the hypothesis that delta outputs produce fewer malformed responses.\n- **Output token reduction**: Compare average output token counts between C and B.\nReport whether Criterion 2 passes or fails.",
            "step4": "**Supplementary Comparison: Cognitive Map Probing (Initial Exploration)**: While the primary decision rule focuses on false-belief revision, also compare A vs B vs C on initial exploration metrics (final correctness, stability) to assess whether the belief-update interface helps even without environmental changes. This provides additional evidence on whether regeneration noise contributes to belief drift during normal exploration.",
            "step5": "**Synthesize Overall Effectiveness Conclusion**: Summarize findings:\n- Does Condition B consistently improve over Condition A? If yes, this supports the hypothesis that regeneration noise / inconsistent conflict handling contributes to ToS belief drift/inertia.\n- Does Condition C improve over or match Condition B with practical benefits (fewer errors, smaller outputs)? If yes, this supports the delta-map format as a practical improvement.\n- If B does not improve over A, the intervention is refuted: drift/inertia is dominated by perception errors rather than map-regeneration errors.\n- Identify any settings where results are mixed or unexpected."
        }
    },
    {
        "category": "Analysis Experiment",
        "title": "Edit Magnitude Analysis — Sparsity of Belief Updates",
        "description": "Analyze the average number of objects updated per step under Conditions B and C during both initial exploration and false-belief revision. If the number of updates per step is small relative to the total number of objects in the map, this supports the 'sparse evidence' premise motivating delta-map updates: most steps only provide evidence about a small subset of objects. This analysis also compares the update patterns between initial exploration (where changes are additive, incorporating new objects) and false-belief revision (where changes involve overwriting existing entries).",
        "steps": {
            "step1": "**Compute Per-Step Edit Counts**: For each scene and each condition (B and C), across both initial exploration and false-belief revision:\n- **Condition B**: Compare consecutive maps $M_{t-1}$ and $M_t$ to count the number of object entries that differ (position or facing changed, or new object added). Implement in `delta_map_updates/analysis/edit_magnitude.py` by computing a set-difference on the JSON maps.\n- **Condition C**: Directly count the number of entries in the delta JSON `updates` field at each step.\nRecord per-step edit counts for all scenes and steps.",
            "step2": "**Aggregate and Visualize Edit Magnitude Statistics**: Compute the following statistics across all 25 scenes:\n- Mean and standard deviation of the number of objects updated per step.\n- Ratio of updates per step to total objects in the map (to measure sparsity).\n- Distribution of edit magnitudes (histogram).\nCompare between initial exploration and false-belief revision phases. Create a box plot or violin plot showing the distribution of per-step edit counts for Conditions B and C across both phases. Also compute the average output token count per step for Conditions A, B, and C to show that delta outputs are shorter."
        }
    },
    {
        "category": "Analysis Experiment",
        "title": "Failure Stratification — Objects With vs Without Facing Direction",
        "description": "Stratify the belief update results by whether objects have a facing direction (orientation) or not. The ToS paper identifies orientation as a key perception bottleneck (facing accuracy is much lower than positional accuracy in vision world). This analysis tests whether the gains from Conditions B and C are larger for objects with facing direction (where drift and inertia are worst) vs objects without facing, and whether the belief-update interface disproportionately helps the orientation component of map correctness.",
        "steps": {
            "step1": "**Classify Objects by Facing Property**: For each of the N=25 scenes, load `meta_data.json` and classify all objects into two groups: (1) objects with a meaningful facing direction (e.g., furniture with a front-facing orientation), and (2) objects without facing direction (or symmetric objects). Also classify the k=4 changed objects in the false-belief task into these groups using `falsebelief_exp.json`. Implement in `delta_map_updates/analysis/failure_stratification.py`.",
            "step2": "**Compute Stratified Metrics**: For each condition (A, B, C) and each object group (with-facing, without-facing), compute:\n- **Cognitive Map Probing**: Final facing accuracy (for with-facing objects only), positional accuracy (both groups).\n- **False-Belief Revision**: Orientation inertia (for with-facing objects), positional inertia (both groups), identification F1 stratified by whether the change involved orientation vs position.\nReport improvements from A→B and B→C separately for each group.",
            "step3": "**Visualize and Interpret Stratified Results**: Create grouped bar charts comparing Conditions A, B, and C across the two object groups. Plot: (1) facing accuracy improvement (A→B→C) for with-facing vs without-facing objects, (2) inertia reduction stratified by change type (orientation change vs position-only change). Summarize whether the belief-update interface disproportionately benefits orientation-sensitive objects (where the original ToS findings showed the worst failures)."
        }
    }
]