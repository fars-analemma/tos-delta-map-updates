UNCERTAINTY MAP PROBING
Methods Text (%) Vision (%) GPT-5.2 41.8 57.0 GEMINI-3 PRO 46.6 64.5 Table 6: Pearson correlation (r) between spatial-belief correctness and downstream evaluation performance. All correlations are significant (p < .001). To probe an agent's ability to model uncertainty, we provide it with a top-down view of the scene in which all objects are removed, and we overlay a set of candidate points. These points are sampled randomly and include both previously observed and unobserved locations. The agent's task is to identify which candidate points remain unobserved, thereby revealing its belief over unseen regions. Representation. The agent receives an empty top down map that shows only the candidate points and its current position, with no objects present. The agent must select the points that have not yet been observed. In the text based world, the top down map is represented as an N × M symbolic grid, where different symbols denote the agent, gates, and candidate points. In the vision based world, all objects are removed and the agent instead receives a top down image of the environment, check examples in Appendix ¶ A.1. We use F 1 to evaluate selected points. We report Uncertainty scores in Table 5. GEMINI-3 PRO models uncertainty better than GPT-5.2 in both text-and vision-based settings. These results help explain the information gain and cognitive map trends in Figure 6. GPT-5.2 achieves higher initial information gain (i.e., it ramps up faster), likely because it quickly commits to an explore-the-doors strategy. However, it generalizes poorly to unobserved regions, reflected by the subsequent plateau in Figure 6: additional steps yield little marginal gain. In contrast, although GEMINI-3 PRO improves more slowly at the beginning, its cognitive map accuracy continues to increase with exploration, suggesting it keeps collecting useful evidence and progressively resolving uncertainty.

[TABLE START]Table 5 : Spatial Belief Quality via Cognitive Map Probing. We measure final map correctness and turn-level perception, local global consistency, stability, self-tracking, and uncertainty in textvs. vision-worlds. ori. for orientation and pos. for position. Across models, vision lags text on all metrics, with the largest drop on orientation and stability. <table><row><cell/><cell>o r i.</cell><cell>p o s .</cell><cell>o v e r a ll o r i.</cell><cell cols="2">p o s . o r i.</cell><cell>p o s .</cell><cell>o r i.</cell><cell>p o s .</cell><cell>o r i.</cell><cell>p o s .</cell></row><row><cell>Methods</cell><cell cols="4">Correctness (%) Perception (%)</cell><cell cols="2">Local↔ Global (%)</cell><cell cols="2">Stability (%)</cell><cell>Self-tracking (%)</cell><cell>Uncertainty (%)</cell></row><row><cell/><cell/><cell/><cell/><cell cols="3">Vision-based World</cell><cell/><cell/></row><row><cell>GPT-5.2</cell><cell cols="3">20.2 42.0 32.2 33.5</cell><cell cols="6">72.4 57.9 58.7 65.4 56.4 93.3</cell><cell>64.7</cell><cell>53.7</cell></row><row><cell cols="4">GEMINI-3 PRO 32.2 62.5 52.1 43.8</cell><cell cols="6">68.5 52.9 68.3 61.8 62.0 98.8</cell><cell>73.9</cell><cell>70.2</cell></row><row><cell/><cell/><cell/><cell/><cell cols="3">Text-based World</cell><cell/><cell/></row><row><cell>GPT-5.2</cell><cell cols="3">91.0 75.1 80.0 100</cell><cell cols="6">86.8 96.4 86.0 96.7 67.6 98.0</cell><cell>86.7</cell><cell>64.5</cell></row><row><cell cols="4">GEMINI-3 PRO 92.5 75.5 81.4 99.9</cell><cell cols="6">88.2 91.6 84.8 90.8 67.7 99.9</cell><cell>85.2</cell><cell>79.2</cell></row></table>[TABLE END]


[IMAGE START]Figure 6 : Figure 6: Accumulated Information Gain and Cognitive Map Correctness over steps.[IMAGE END]
