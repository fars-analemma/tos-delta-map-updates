EVALUATION AND ANALYSIS
We evaluate a set of state-of-the-art proprietary and open-source foundation models. They are evaluated on both passive and active settings described in § 3.4. Unless otherwise specified for ablations, all experiments use three connected 6 × 6 rooms with 4 objects in each (total 12 objects). To enable a like-for-like comparison between the text and vision settings, we instantiate identical room layouts across modalities. We use 384 × 384 images in the vision setting. We generate 100 scenes and create three questions per task per scene, yielding 3 × 9 × 100 = 2700 questions per setting. We mainly evaluate six foundation models: GPT-5.2 (OpenAI, 2025), GEMINI-3 PRO (Google, 2025), CLAUDE-4.5-SONNET (Anthropic, 2025), GLM-4.6V ~\cite{b61}, QWEN3-VL ~\cite{b1}) (235B-A22B-Thinking), and INTERNVL-3.5 ~\cite{b48}) (241B-A28B). For closed-source reasoning models GPT-5.2, GEMINI-3 PRO, and CLAUDE-4.5-SONNET, we set the temperature to 1 and the maximum number of tokens to 32768. For all other models, we set the temperature to 0. INTERNVL-3.5 supports at most 10 images, so we omit it for the vision-based world setting. Active Exploration Results. We evaluate models as active agents, where they must autonomously explore the environment to build their spatial belief and terminate the exploration process by their own. This setting tests the full THEORY OF SPACE pipeline, requiring the agent to simultaneously plan an efficient information-gathering trajectory, integrate observations, and maintain a coherent cognitive map under uncertainty. The agent's performance is measured by its Exploration Efficiency as shown in § 3.3 and its final accuracy on the downstream spatial tasks. The agent has a maximum of 20 exploration steps. the course of the exploration turns. GPT-5.2 acquires substantial information early on, but its rate of gain slows in later turns, resulting in lower cumulative information gain than GEMINI-3 PRO and CLAUDE-4.5 SONNET. Moreover, none of the models achieves full coverage relative to the proxy agent. We benchmarked three human subjects across five text and five vision scenes. Humans consistently outperformed foundation models in both domains, particularly in vision. Intuitively, humans scored higher in vision than text as visual information is easier to process. With tools, they achieved near-perfect accuracy Passive Exploration Results. We evaluate models on trajectories generated by rule-based proxy agent to understand a model's core spatial reasoning ability regardless of its exploration strategy. The performance of various models in both text-based and vision-based environments is summarized in Table 3. As evaluated, the results show a clear separation: GPT-5.2 and GEMINI-3 PRO lead by a wide margin over other systems, particularly open-source models. A substantial modality gap persists, with text performance far better than vision performance for all models. Key Findings: Modality Gap • Modality Gap Exists: text significantly outperforms vision. Overall, active accuracies underperform the passive setting. Incomplete exploration leads to drops: Figure 4 shows that GPT-5.2 gathers information quickly but often terminates prematurely, leaving uncertainty and lowering active scores relative to passive. Compared to the strategist proxy, which achieves full certainty, models remain less thorough. A second critical disparity is the efficiency gap. In the vision domain, the SCOUT proxy reaches target coverage in ≈ 9 steps, whereas autonomous models expend significantly more actions with no performance benefit. This inefficiency is further highlighted in the text domain. While our primary text experiments utilize the STRATEGIST proxy for maximum coverage, we additionally evaluated the SCOUT proxy in text world. The text-based SCOUT similarly averages ≈ 9 steps. When following these concise trajectories, GPT-5.2 and GEMINI-3 PRO achieve accuracies of 83.9 and 86.7, respectively. These scores surpass their active exploration performance (72.0, 81.5 for GPT-5.2 and GEMINI-3 PRO, as in Table 2), demonstrating that models perform better when guided by a short, efficient proxy path than when exploring autonomously.  Exploration Pattern Manual inspection of agent exploration histories reveals distinct behavioral patterns. For GPT-5.2, the active-passive performance gap stems from unsystematic exploration. Specifically, the agent tends to prioritize any newly discovered door, immediately jumping to inspect it and often leaving the current room partially unexplored. This is compounded by object omission and path redundancy. In contrast, GEMINI-3 PRO adopts a more methodical "rotate-and-scan" strategy, scanning its surroundings before transitioning to new rooms, which is a behavior mirroring the SCOUT proxy agent. Further examples are provided in Appendix ¶ C.

[TABLE START]Table 2 : Exploitation Performance (%) of Belief Construction via Active Exploration. Models autonomously plan actions and are evaluated on exploration cost, route-level reasoning, and surveylevel reasoning across text-and vision-based environments. GEMINI-3 PRO leads every task and all reasoning metrics, while GPT-5.2 achieves the lowest exploration cost in text-world. Humans outperform in both settings, especially in vision. ⋆ Humans can use instruments such as protractors and compasses to infer object positions precisely. <table><row><cell/><cell/><cell/><cell/><cell/><cell>a c t2 v ie w</cell><cell>v ie w 2 a c t</cell><cell>a ll o c .m a p</cell><cell>m e n t. ro t</cell><cell>lo c 2 v ie w</cell><cell>v ie w 2 lo c</cell><cell/></row><row><cell/><cell/><cell>Static (S)</cell><cell/><cell cols="2">Dynamic (D)</cell><cell/><cell>Static (S)</cell><cell cols="3">Dynamic (D)</cell><cell/></row><row><cell>Methods</cell><cell>Avg.step</cell><cell/><cell/><cell>Route</cell><cell/><cell/><cell/><cell cols="2">Survey</cell><cell/><cell>Avg.</cell></row><row><cell/><cell/><cell/><cell cols="3">Vision-based World</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Proprietary Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GPT-5.2</cell><cell>17.2</cell><cell>40.0</cell><cell>36.7</cell><cell>56.2</cell><cell>43.8</cell><cell>40.3</cell><cell>43.4</cell><cell>59.7</cell><cell>56.9</cell><cell>37.8</cell><cell>46.0</cell></row><row><cell>GEMINI-3 PRO</cell><cell>13.6</cell><cell>56.3</cell><cell>36.7</cell><cell>68.2</cell><cell>47.2</cell><cell>54.0</cell><cell>63.5</cell><cell>73.0</cell><cell>65.4</cell><cell>52.2</cell><cell>57.3</cell></row><row><cell>CLAUDE-4.5 SONNET</cell><cell>19.6</cell><cell>23.7</cell><cell>23.3</cell><cell>18.7</cell><cell>33.3</cell><cell>10.7</cell><cell>37.4</cell><cell>34.7</cell><cell>33.7</cell><cell>50.9</cell><cell>29.6</cell></row><row><cell>Open-source Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GLM-4.6V</cell><cell>15.0</cell><cell>15.8</cell><cell>18.5</cell><cell>3.3</cell><cell>14.0</cell><cell>0.7</cell><cell>18.9</cell><cell>8.0</cell><cell>18.5</cell><cell>31.8</cell><cell>14.4</cell></row><row><cell>QWEN3-VL</cell><cell>16.3</cell><cell>16.8</cell><cell>23.3</cell><cell>13.4</cell><cell>24.8</cell><cell>5.7</cell><cell>25.8</cell><cell>16.3</cell><cell>21.5</cell><cell>43.7</cell><cell>21.3</cell></row><row><cell>HUMAN</cell><cell>9.8</cell><cell cols="5">94.5 100.0 100.0 100.0 93.4</cell><cell cols="3">93.4 100.0 100.0</cell><cell>86.7</cell><cell>96.4</cell></row><row><cell>HUMAN WITH TOOL ⋆</cell><cell>11.1</cell><cell cols="5">100.0 100.0 100.0 100.0 97.8</cell><cell cols="3">100.0 100.0 100.0</cell><cell>93.4</cell><cell>99.0</cell></row><row><cell/><cell/><cell/><cell cols="3">Text-based World</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Proprietary Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GPT-5.2</cell><cell>11.4</cell><cell>68.8</cell><cell>70.5</cell><cell>80.3</cell><cell>71.0</cell><cell>53.7</cell><cell>77.9</cell><cell>81.0</cell><cell>79.1</cell><cell>66.0</cell><cell>72.0</cell></row><row><cell>GEMINI-3 PRO</cell><cell>13.5</cell><cell>78.0</cell><cell>79.2</cell><cell>90.6</cell><cell>75.3</cell><cell>76.3</cell><cell>81.0</cell><cell>94.0</cell><cell>83.3</cell><cell>76.2</cell><cell>81.5</cell></row><row><cell>CLAUDE-4.5 SONNET</cell><cell>18.7</cell><cell>65.3</cell><cell>65.3</cell><cell>79.0</cell><cell>62.7</cell><cell>51.7</cell><cell>68.8</cell><cell>76.3</cell><cell>57.0</cell><cell>67.0</cell><cell>65.9</cell></row><row><cell>Open-source Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GLM-4.6V</cell><cell>14.5</cell><cell>20.8</cell><cell>19.7</cell><cell>12.7</cell><cell>21.8</cell><cell>3.7</cell><cell>13.9</cell><cell>9.3</cell><cell>22.7</cell><cell>26.2</cell><cell>16.8</cell></row><row><cell>INTERNVL-3.5</cell><cell>15.0</cell><cell>28.8</cell><cell>44.8</cell><cell>26.0</cell><cell>36.8</cell><cell>7.3</cell><cell>31.0</cell><cell>27.7</cell><cell>33.8</cell><cell>38.9</cell><cell>30.6</cell></row><row><cell>QWEN3-VL</cell><cell>14.1</cell><cell>32.3</cell><cell>45.7</cell><cell>48.2</cell><cell>33.3</cell><cell>11.7</cell><cell>36.4</cell><cell>34.7</cell><cell>35.7</cell><cell>49.9</cell><cell>36.8</cell></row><row><cell>HUMAN</cell><cell>10.8</cell><cell>87.8</cell><cell cols="3">82.1 100.0 85.5</cell><cell>86.8</cell><cell cols="3">66.6 100.0 95.6</cell><cell>75.8</cell><cell>86.7</cell></row><row><cell>HUMAN WITH TOOL ⋆</cell><cell>12.8</cell><cell cols="5">100.0 100.0 100.0 100.0 100.0</cell><cell cols="3">100.0 100.0 100.0</cell><cell>91.2</cell><cell>99.0</cell></row></table>[TABLE END]


[TABLE START]Table 3 : Exploitation <table><row><cell/><cell>d ir e c ti o n</cell><cell>p e r s p .t a k e</cell><cell>p e r c .d e c</cell><cell>a c t2 v ie w</cell><cell>v ie w 2 a c t</cell><cell>a ll o c .m a p</cell><cell>m e n t. r o t</cell><cell>lo c 2 v ie w</cell><cell>v ie w 2 lo c</cell><cell/></row><row><cell/><cell>Static (S)</cell><cell/><cell cols="2">Dynamic (D)</cell><cell/><cell>Static (S)</cell><cell cols="3">Dynamic (D)</cell><cell/></row><row><cell>Methods</cell><cell/><cell cols="2">Route</cell><cell/><cell/><cell/><cell cols="2">Survey</cell><cell/><cell>Avg.</cell></row><row><cell/><cell/><cell/><cell cols="3">Vision-based World</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Proprietary Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GPT-5.2</cell><cell>47.3</cell><cell>35.0</cell><cell>63.9</cell><cell>54.5</cell><cell>49.3</cell><cell>64.8</cell><cell>83.3</cell><cell>50.3</cell><cell>65.6</cell><cell>57.1</cell></row><row><cell>GEMINI-3 PRO</cell><cell>63.8</cell><cell>36.3</cell><cell>57.5</cell><cell>49.0</cell><cell>58.0</cell><cell>67.2</cell><cell>85.3</cell><cell>70.4</cell><cell>57.0</cell><cell>60.5</cell></row><row><cell>CLAUDE-4.5 SONNET</cell><cell>47.3</cell><cell>33.5</cell><cell>37.7</cell><cell>40.8</cell><cell>15.7</cell><cell>54.8</cell><cell>58.3</cell><cell>44.7</cell><cell>54.8</cell><cell>43.1</cell></row><row><cell>Open-source Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GLM-4.6V</cell><cell>11.5</cell><cell>24.5</cell><cell>4.7</cell><cell>19.0</cell><cell>2.7</cell><cell>22.9</cell><cell>11.7</cell><cell>20.0</cell><cell>33.6</cell><cell>16.7</cell></row><row><cell>QWEN3-VL</cell><cell>20.8</cell><cell>28.3</cell><cell>22.7</cell><cell>16.7</cell><cell>4.7</cell><cell>33.2</cell><cell>21.7</cell><cell>27.3</cell><cell>40.8</cell><cell>24.9</cell></row><row><cell/><cell/><cell/><cell cols="3">Text-based World</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Proprietary Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GPT-5.2</cell><cell>84.5</cell><cell>88.2</cell><cell>97.0</cell><cell>89.0</cell><cell>76.0</cell><cell>96.3</cell><cell>98.3</cell><cell>94.8</cell><cell>89.2</cell><cell>90.4</cell></row><row><cell>GEMINI-3 PRO</cell><cell>82.7</cell><cell>92.7</cell><cell>97.0</cell><cell>87.5</cell><cell>75.7</cell><cell>86.2</cell><cell>91.3</cell><cell>85.7</cell><cell>80.0</cell><cell>86.5</cell></row><row><cell>CLAUDE-4.5 SONNET</cell><cell>73.0</cell><cell>80.7</cell><cell>90.7</cell><cell>77.7</cell><cell>59.0</cell><cell>76.9</cell><cell>74.3</cell><cell>59.2</cell><cell>70.7</cell><cell>73.6</cell></row><row><cell>Open-source Models</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>GLM-4.6V</cell><cell>22.3</cell><cell>39.8</cell><cell>25.0</cell><cell>25.3</cell><cell>4.7</cell><cell>21.2</cell><cell>9.0</cell><cell>27.0</cell><cell>35.7</cell><cell>23.4</cell></row><row><cell>INTERNVL-3.5</cell><cell>36.7</cell><cell>67.8</cell><cell>42.7</cell><cell>41.2</cell><cell>8.7</cell><cell>37.3</cell><cell>19.3</cell><cell>38.7</cell><cell>43.8</cell><cell>37.4</cell></row><row><cell>QWEN3-VL</cell><cell>40.8</cell><cell>69.3</cell><cell>56.5</cell><cell>50.0</cell><cell>17.7</cell><cell>42.8</cell><cell>40.3</cell><cell>42.5</cell><cell>54.6</cell><cell>45.6</cell></row></table>[TABLE END]


[IMAGE START]Figure 4 : Figure 4: Accumulated information gain over exploration steps in the text world.Different Room Settings. For the two best-performing models, GPT-5.2 and GEMINI-3 PRO, we further evaluate reasoning and exploration under different room configurations: a four-room setting and two three-room settings. In the four-room setting, the main room connects to the other three rooms. Table4reports results across different room settings. As the number of rooms increases, exploration cost rises accordingly. For both GPT-5.2 and GEMINI-3 PRO, performance declines as the room number increases, and the active-passive performance gap widens with room number. Moreover, GEMINI-3 PRO requires nearly the same number of exploration steps in the text-only and vision-based environments. Detailed results are in Appendix ¶ B.Key Findings: Active Exploration as the Bottleneck• Performance and Efficiency Deficit: Active agents score lower than reasoning on rule based program histories, and explore less efficiently than the program. • Incomplete Coverage: Active agent fails to achieve complete information coverage.• Complexity-Widened Gap: The active versus passive difference grows with environment scale; GEMINI-3 PRO degrades least.[IMAGE END]


Section references:
[b1]: Shuai Bai, Yuxuan Cai, Ruizhe Chen, Ke Zhu. Qwen3-vl: The next generation multimodal llm from qwen / alibaba cloud. (2025). Qwen3-vl: The next generation multimodal llm from qwen / alibaba cloud
[b48]: Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. (2025). Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency
[b61]: A Zhipu. Available online; native multimodal vision + reasoning model (128k context) among open-source LLMs. (2025). / model release