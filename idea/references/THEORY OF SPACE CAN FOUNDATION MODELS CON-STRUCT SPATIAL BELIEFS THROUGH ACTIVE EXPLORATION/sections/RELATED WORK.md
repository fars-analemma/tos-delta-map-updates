RELATED WORK
Passive Spatial Reasoning. Early paradigms treat spatial reasoning as static inference: given a textual description, agents answer relational queries ~\cite{b50,b41,b33,b27}. Other benchmarks probe understanding from a single image, asking for relative directions, topological relations, or metric attributes ~\cite{b30,b11,b6,b5,b29,b22}. Multi-view and video benchmarks raise difficulty by requiring cross-view integration, egocentric-allocentric conversion, and temporal consistency (Yang et al., 2025c;~\cite{b54,b52,b58,b14}Zhou et al., 2025b). Recent works explicitly adopt cognitive maps: VSI-Bench (Yang et al., 2025a) shows map formation improves video QA, and MindCube ~\cite{b59} demonstrates that predicting layouts boosts multi-view reasoning. While informative, these benchmarks remain disembodied, as agents reason only over pre-collected trajectories. Active Exploration for Spatial Understanding. Research has also examined agents that actively explore, but their exploration is usually tied to task-specific goals rather than building a general spatial belief. Embodied question answering benchmarks evaluate agents by whether they can gather evidence to answer questions ~\cite{b9,b17,b31,b15,b40}. Instruction-following settings extend household tasks to long horizons and realistic scenes, often with dialog or language grounding (Shridhar et al., 2020b;~\cite{b23}Shridhar et al., 2020a;~\cite{b38,b37,b13}. Navigation benchmarks stress path execution and generalization across diverse environments ~\cite{b0,b20,b26,b25,b35,b49,b60}. Spatial reference tasks focus on grounding natural-language descriptions in embodied search ~\cite{b39}Zhou et al., 2025a), and manipulation ~\cite{b21}Mees et al., 2022;~\cite{b45,b53}. While existing benchmarks incorporate active perception, they largely rely on task-driven foraging. This paradigm conflates the efficiency of environmental exploration with downstream task performance, often fostering brittle spatial representations that lack generalizability ~\cite{b3}. Beyond the above task-driven active exploration, EXCALIBURZhu et al. ( 2023) also considers task-agnostic exploration, but its RL training can induce goal leakage and encodes maps implicitly in policy weights. In contrast, we study zero-shot foundation-model agents with no environment-specific training for task-agnostic exploration, emphasizing exploration efficiency via minimal-cost uncertainty reduction (rather than coverage), and evaluating not only task success but also the belief construction process via explicit belief probing.

Section references:
[b0]: Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. (2018-09). the IEEE conference on computer vision and pattern recognition
[b11]: Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou. Internspatial: A comprehensive dataset for spatial reasoning in vision-language models. (2025). Internspatial: A comprehensive dataset for spatial reasoning in vision-language models
[b13]: Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav Sukhatme. Dialfred: Dialogue-enabled agents for embodied instruction following. (2022)
[b14]: Mohsen Gholami, Ahmad Rezaei, Yong Zhou Weimin, Mohammad Zhang. Spatial reasoning with vision-language models in ego-centric multi-view scenes. (2025). Spatial reasoning with vision-language models in ego-centric multi-view scenes
[b15]: Muhammad Fadhil, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Jai Bandi, Navid Krishna, Oriana Kayhani, David Peltzer, Amirreza Fan, Sung-Kyun Shaban, Mykel Kim, Ali Kochenderfer, Shayegan Akbar Agha-Mohammadi. Enter the mind palace: Reasoning and planning for long-term active embodied question answering. (2025). Enter the mind palace: Reasoning and planning for long-term active embodied question answering
[b17]: Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi. Iqa: Visual question answering in interactive environments. (2018). the IEEE conference on computer vision and pattern recognition
[b20]: Vihan Jain, Gabriel Magalhães, Alexander Ku, Ashish Vaswani, Eugene Ie, Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. (2019). the 57th Annual Meeting of the Association for Computational Linguistics (ACL)
[b21]: Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan. Vima: General robot manipulation with multimodal prompts. (2023). the 40th International Conference on Machine Learning (ICML)
[b22]: Amita Kamath, Jack Hessel, Kai-Wei Chang. What's" up" with vision-language models? investigating their struggle with spatial reasoning. (2023). What's" up" with vision-language models? investigating their struggle with spatial reasoning
[b23]: Taewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, Jonghyun Choi. Realfred: An embodied instruction following benchmark in photo-realistic environments. (2024). ECCV
[b25]: Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. (2020). European Conference on Computer Vision
[b26]: Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. (2020). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
[b27]: Fangjun Li, David Hogg, Anthony Cohn. Reframing spatial reasoning evaluation in language models: A real-world simulation benchmark for qualitative reasoning. (2024). Reframing spatial reasoning evaluation in language models: A real-world simulation benchmark for qualitative reasoning
[b29]: Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. (2024). Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models
[b3]: Elizabeth Bonawitz, Patrick Shafto, Hyowon Gweon, Elizabeth Noah D Goodman, Laura Spelke. The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery. (2011)
[b30]: Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso De Melo, Alan Yuille. 3dsrbench: A comprehensive 3d spatial reasoning benchmark. (2024). 3dsrbench: A comprehensive 3d spatial reasoning benchmark
[b31]: Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud. Embodied question answering in the era of foundation models. (2024). the IEEE/CVF conference on computer vision and pattern recognition
[b33]: Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning. A textual question answering benchmark for spatial reasoning. (2021). A textual question answering benchmark for spatial reasoning
[b35]: Khanh Nguyen, Hal Daumé. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. (2019). Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning
[b37]: Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. (2022). the AAAI Conference on Artificial Intelligence
[b38]: Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba. Virtualhome: Simulating household activities via programs. (2018). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
[b39]: Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Wang, Chunhua Shen, Anton Van Den. Reverie: Remote embodied visual referring expression in real indoor environments. (2019). Reverie: Remote embodied visual referring expression in real indoor environments
[b40]: Jaden Allen Z Ren, Anushri Clark, Masha Dixit, Anirudha Itkina, Dorsa Majumdar. Explore until confident: Efficient exploration for embodied question answering. (2024). Explore until confident: Efficient exploration for embodied question answering
[b41]: Zhengxiang Shi, Qiang Zhang, Aldo Lipani. Stepgame: A new benchmark for robust multihop spatial reasoning in texts. (2022). the AAAI conference on artificial intelligence
[b45]: Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, Aleksandra Faust, David Hsu, Gerhard Neumann. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. (2022-11). the 5th Conference on Robot Learning
[b49]: Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu. Divscene: Benchmarking lvlms for object navigation with diverse scenes and objects. (2024). Divscene: Benchmarking lvlms for object navigation with diverse scenes and objects
[b5]: Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. (2024). the IEEE/CVF Conference on Computer Vision and Pattern Recognition
[b50]: Jason Weston, Antoine Bordes, Sumit Chopra, Alexander Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. (2015). Towards ai-complete question answering: A set of prerequisite toy tasks
[b52]: Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, Weidi Xie. Towards unified evaluation for multimodal spatial understanding. (2025). Towards unified evaluation for multimodal spatial understanding
[b53]: Yue Wu, Xuan Tang, Tom Mitchell, Yuanzhi Li. A benchmark for llms as intelligent agents. (2023). A benchmark for llms as intelligent agents
[b54]: Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. (2025). Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models
[b58]: Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Ruoyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, Yi Ma. Seeing from another perspective: Evaluating multi-view understanding in mllms. (2025). Seeing from another perspective: Evaluating multi-view understanding in mllms
[b59]: Qineng Baiqiao Yin, Pingyue Wang, Jianshu Zhang, Kangrui Zhang, Zihan Wang, Jieyu Wang, Keshigeyan Zhang, Han Chandrasegaran, Ranjay Liu. Spatial mental modeling from limited views. (2025). Spatial mental modeling from limited views
[b6]: An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. (2024)
[b60]: Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, Jincai Huang. Cityeqa: A hierarchical llm agent on embodied question answering benchmark in city space. (2025). Cityeqa: A hierarchical llm agent on embodied question answering benchmark in city space
[b9]: Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra. Embodied question answering. (2018). the IEEE conference on computer vision and pattern recognition