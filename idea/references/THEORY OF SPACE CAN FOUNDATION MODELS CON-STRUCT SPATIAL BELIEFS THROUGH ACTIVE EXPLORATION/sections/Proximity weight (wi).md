Proximity weight (wi)
. Here cos θ i is large when the remaining error after updating still points toward the obsolete location, while w i downweights such alignment when the belief has moved far from b old i . We set σ to a dynamic noise scale: the RMS localization error on the first re-observed unchanged objects during re-exploration; ϵ ensures numerical stability. Under unbiased updating, E[s pos i ] ≈ 0, whereas s pos i > 0 indicates systematic pull toward the obsolete prior. For orientation shifts, we measure inertia via s ori i = 1 ϕ new i = ϕ old i , where ϕ denotes the predicted orientation. It flags failures to overwrite the obsolete facing direction. Table 7 corroborates the modality gap observed in previous sections: vision-based agents significantly underperform their text-based counterparts. This performance drop is characterized by increased exploration redundancy and lower accuracy in identifying changed objects. Notably, while belief inertia persists across both modalities, it is markedly more severe in vision-based agents, particularly regarding object orientation. Vision models frequently fail to overwrite their initial spatial memory, persisting with obsolete facing estimates despite new visual evidence. This also suggests that fine-grained orientation estimation remains a critical bottleneck for visual spatial reasoning.