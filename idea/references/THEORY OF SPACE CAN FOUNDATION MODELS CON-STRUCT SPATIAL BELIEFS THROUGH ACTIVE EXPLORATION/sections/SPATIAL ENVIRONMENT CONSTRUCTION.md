SPATIAL ENVIRONMENT CONSTRUCTION
To ensure controlled experimentation, we procedurally generate multi-room indoor layouts on an N × M grid. Each scene is populated with n indoor objects, each assigned a 2D integer coordinate and a cardinal orientation from (N, S, E, W). The agent begins at a random position, is informed of the total number of rooms and the names of all objects in the scene, and then starts exploration. Following the Gym-style interface ~\cite{b4}, we define procedurally generated, highly scalable environments in which each random seed deterministically instantiates a distinct multi-room layout. Action Space in the Environment. The agent's interaction with the world is designed to focus on high-level decision-making rather than low-level motor control: Goto to move directly to a currently visible object; Rotate to turn in place by 90 • , 180 • , or 270 • ; Observe to perceive visible objects in the 90 • field of view; and Query to obtain a visible object's absolute 2D coordinates. We additionally assign costs of 1 to Observe and 2 to Query, encouraging Query to be used only when necessary to resolve ambiguity. However, across all models Query is invoked only rarely, so we restrict attention to Observe and measure exploration efficiency by step count instead of action cost. Observation Feedback from a Text-Vision Parallel Environment. We offer both text-based and vision-based environments, enabling diagnostic analysis of spatial reasoning. Each Observe action returns both textual and visual feedback from a 90 • field of view. The Text World provides symbolic observations with discrete bins for direction and distance (e.g., "chair is front-left and near", detailed below), isolating pure spatial reasoning. The Visual World instead supplies ego-centric RGB images rendered in ThreeDWorld (Gan et al., 2021) with Objaverse assets (Deitke et al., 2022), requiring perception to recover spatial relations. To calibrate perception in the visual setting, we provide two reference images, indicating unit distance (1 grid unit) / angle (a 22.5 • angular cone), and showing all objects with their names and canonical "front" orientation, respectively. Details are shown in Appendix ¶ A.1 Spatial Relation Representation. To ensure that agents perceive and communicate about space using a consistent language across tasks and modalities, we discretize spatial relationships for directions and distances. For allocentric direction, we discretize into eight 45 • bins aligned with the four cardinal and four intercardinal directions, denoted compactly as {N, NE, E, SE, S, SW, W, NW}. Each bin spans 45 • around its heading (e.g., N = [-22.5 • , 22.5 • )). For egocentric direction, within a 90 • forward field of view (FOV), we use five labels: front-left [-45 • , -22.5 • ), front-slight-left [-22.5 • , 0), front 0 • , front-slight-right (0, 22.5 • ], and front-right (22.5 • , 45 • ]. For distance, measured in map units independent of direction, we define six bins: same = 0, near (0, 2], mid (2, 4], slightly far (4, 8], far (8, 16], and very far (16, 32].

Section references:
[b4]: Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba. . (2016)