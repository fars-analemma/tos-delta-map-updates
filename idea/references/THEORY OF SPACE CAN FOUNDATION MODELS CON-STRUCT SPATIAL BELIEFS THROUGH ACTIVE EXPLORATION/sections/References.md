References
[b0]: Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. (2018-09). the IEEE conference on computer vision and pattern recognition
[b1]: Shuai Bai, Yuxuan Cai, Ruizhe Chen, Ke Zhu. Qwen3-vl: The next generation multimodal llm from qwen / alibaba cloud. (2025). Qwen3-vl: The next generation multimodal llm from qwen / alibaba cloud
[b2]: Simon Baron-Cohen, Alan Leslie, Uta Frith. Does the autistic child have a "theory of mind. (1985)
[b3]: Elizabeth Bonawitz, Patrick Shafto, Hyowon Gweon, Elizabeth Noah D Goodman, Laura Spelke. The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery. (2011)
[b4]: Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba. . (2016)
[b5]: Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. (2024). the IEEE/CVF Conference on Computer Vision and Pattern Recognition
[b6]: An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. (2024)
[b7]: R Elizabeth, William Chrastil. Active and passive contributions to spatial learning. (2012)
[b8]: R Elizabeth, William Chrastil. Active and passive spatial learning in human navigation: acquisition of survey knowledge. (2013)
[b9]: Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra. Embodied question answering. (2018). the IEEE conference on computer vision and pattern recognition
[b10]: Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vanderbilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi. A universe of annotated 3d objects. (2022). A universe of annotated 3d objects
[b11]: Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou. Internspatial: A comprehensive dataset for spatial reasoning in vision-language models. (2025). Internspatial: A comprehensive dataset for spatial reasoning in vision-language models
[b12]: Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Michael Lingelbach, Aidan Curtis, Kevin Feigelis, Daniel Bear, Dan Gutfreund, David Cox, Antonio Torralba, James Dicarlo, Joshua Tenenbaum, Josh Mcdermott, L Daniel. Threedworld: A platform for interactive multi-modal physical simulation. (2021). Threedworld: A platform for interactive multi-modal physical simulation
[b13]: Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav Sukhatme. Dialfred: Dialogue-enabled agents for embodied instruction following. (2022)
[b14]: Mohsen Gholami, Ahmad Rezaei, Yong Zhou Weimin, Mohammad Zhang. Spatial reasoning with vision-language models in ego-centric multi-view scenes. (2025). Spatial reasoning with vision-language models in ego-centric multi-view scenes
[b15]: Muhammad Fadhil, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Jai Bandi, Navid Krishna, Oriana Kayhani, David Peltzer, Amirreza Fan, Sung-Kyun Shaban, Mykel Kim, Ali Kochenderfer, Shayegan Akbar Agha-Mohammadi. Enter the mind palace: Reasoning and planning for long-term active embodied question answering. (2025). Enter the mind palace: Reasoning and planning for long-term active embodied question answering
[b16]: Gemini 3 pro: Model card. Model-Cards/Gemini-3-Pro-Model-Card.pdf, November 2025. Model card
[b17]: Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi. Iqa: Visual question answering in interactive environments. (2018). the IEEE conference on computer vision and pattern recognition
[b18]: Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, Edvard I Moser. Microstructure of a spatial map in the entorhinal cortex. (2005)
[b19]: Richard Held, Alan Hein. Movement-produced stimulation in the development of visually guided behavior. (1963)
[b20]: Vihan Jain, Gabriel Magalhães, Alexander Ku, Ashish Vaswani, Eugene Ie, Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. (2019). the 57th Annual Meeting of the Association for Computational Linguistics (ACL)
[b21]: Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan. Vima: General robot manipulation with multimodal prompts. (2023). the 40th International Conference on Machine Learning (ICML)
[b22]: Amita Kamath, Jack Hessel, Kai-Wei Chang. What's" up" with vision-language models? investigating their struggle with spatial reasoning. (2023). What's" up" with vision-language models? investigating their struggle with spatial reasoning
[b23]: Taewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, Jonghyun Choi. Realfred: An embodied instruction following benchmark in photo-realistic environments. (2024). ECCV
[b24]: Markus Knauff, Leandra Bucher. Antje Krumnack, and Jelica Nejasmic. Spatial belief revision. (2013)
[b25]: Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. (2020). European Conference on Computer Vision
[b26]: Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. (2020). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
[b27]: Fangjun Li, David Hogg, Anthony Cohn. Reframing spatial reasoning evaluation in language models: A real-world simulation benchmark for qualitative reasoning. (2024). Reframing spatial reasoning evaluation in language models: A real-world simulation benchmark for qualitative reasoning
[b28]: Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu. Embodied agent interface: Benchmarking llms for embodied decision making. (2025). Embodied agent interface: Benchmarking llms for embodied decision making
[b29]: Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. (2024). Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models
[b30]: Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso De Melo, Alan Yuille. 3dsrbench: A comprehensive 3d spatial reasoning benchmark. (2024). 3dsrbench: A comprehensive 3d spatial reasoning benchmark
[b31]: Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud. Embodied question answering in the era of foundation models. (2024). the IEEE/CVF conference on computer vision and pattern recognition
[b32]: Oier Mees, Lukas Hermann, Erick Rosete-Beas, Wolfram Burgard. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. 2022. Also available as
[b33]: Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning. A textual question answering benchmark for spatial reasoning. (2021). A textual question answering benchmark for spatial reasoning
[b34]: R Daniel. A new framework for understanding the acquisition of spatial knowledge in large-scale environments. Spatial and temporal reasoning in geographic information systems. (1998). A new framework for understanding the acquisition of spatial knowledge in large-scale environments. Spatial and temporal reasoning in geographic information systems
[b35]: Khanh Nguyen, Hal Daumé. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. (2019). Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning
[b36]: O' John, Jonathan Keefe. The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat. (1971)
[b37]: Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. (2022). the AAAI Conference on Artificial Intelligence
[b38]: Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba. Virtualhome: Simulating household activities via programs. (2018). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
[b39]: Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Wang, Chunhua Shen, Anton Van Den. Reverie: Remote embodied visual referring expression in real indoor environments. (2019). Reverie: Remote embodied visual referring expression in real indoor environments
[b40]: Jaden Allen Z Ren, Anushri Clark, Masha Dixit, Anirudha Itkina, Dorsa Majumdar. Explore until confident: Efficient exploration for embodied question answering. (2024). Explore until confident: Efficient exploration for embodied question answering
[b41]: Zhengxiang Shi, Qiang Zhang, Aldo Lipani. Stepgame: A new benchmark for robust multihop spatial reasoning in texts. (2022). the AAAI conference on artificial intelligence
[b42]: Mani Shridhar, Roozbeh Mottaghi, Yonatan Bisk, Luke Zettlemoyer, Dieter Fox. Alfworld: Aligning text and embodied environments for interactive task learning. (2020). Alfworld: Aligning text and embodied environments for interactive task learning
[b43]: Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. (2020). the IEEE/CVF conference on computer vision and pattern recognition
[b44]: Alan Siegel, Sheldon White. The development of spatial representations of large-scale environments. (1975)
[b45]: Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, Aleksandra Faust, David Hsu, Gerhard Neumann. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. (2022-11). the 5th Conference on Robot Learning
[b46]: A Holly, Barbara Taylor. Spatial mental models derived from survey and route descriptions. (1992)
[b47]: Cognitive maps in rats and men. (1948)
[b48]: Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. (2025). Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency
[b49]: Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu. Divscene: Benchmarking lvlms for object navigation with diverse scenes and objects. (2024). Divscene: Benchmarking lvlms for object navigation with diverse scenes and objects
[b50]: Jason Weston, Antoine Bordes, Sumit Chopra, Alexander Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. (2015). Towards ai-complete question answering: A set of prerequisite toy tasks
[b51]: Heinz Wimmer, Josef Perner. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. (1983)
[b52]: Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, Weidi Xie. Towards unified evaluation for multimodal spatial understanding. (2025). Towards unified evaluation for multimodal spatial understanding
[b53]: Yue Wu, Xuan Tang, Tom Mitchell, Yuanzhi Li. A benchmark for llms as intelligent agents. (2023). A benchmark for llms as intelligent agents
[b54]: Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. (2025). Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models
[b55]: Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. (2025). the Computer Vision and Pattern Recognition Conference
[b56]: Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang. Embodiedbench: Comprehensive benchmarking multi-modal large language models for visiondriven embodied agents. (2025). Embodiedbench: Comprehensive benchmarking multi-modal large language models for visiondriven embodied agents
[b57]: Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue. Mmsi-bench: A benchmark for multi-image spatial intelligence. (2025). Mmsi-bench: A benchmark for multi-image spatial intelligence
[b58]: Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Ruoyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, Yi Ma. Seeing from another perspective: Evaluating multi-view understanding in mllms. (2025). Seeing from another perspective: Evaluating multi-view understanding in mllms
[b59]: Qineng Baiqiao Yin, Pingyue Wang, Jianshu Zhang, Kangrui Zhang, Zihan Wang, Jieyu Wang, Keshigeyan Zhang, Han Chandrasegaran, Ranjay Liu. Spatial mental modeling from limited views. (2025). Spatial mental modeling from limited views
[b60]: Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, Jincai Huang. Cityeqa: A hierarchical llm agent on embodied question answering benchmark in city space. (2025). Cityeqa: A hierarchical llm agent on embodied question answering benchmark in city space
[b61]: A Zhipu. Available online; native multimodal vision + reasoning model (128k context) among open-source LLMs. (2025). / model release
[b62]: Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, Shanghang Zhang. Towards spatial referring with reasoning in vision-language models for robotics. (2025). Towards spatial referring with reasoning in vision-language models for robotics
[b63]: Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Eric Wang, Achuta Kadambi. Vlm4d: Towards spatiotemporal awareness in vision language models. (2025). Vlm4d: Towards spatiotemporal awareness in vision language models
[b64]: Raghav Hao Zhu, So Kapoor, Winson Yeon Min, Jiatai Han, Kaiwen Li, Graham Geng, Yonatan Neubig, Aniruddha Bisk, Luca Kembhavi. Excalibur: Encouraging and evaluating embodied exploration. (2023). the IEEE/CVF Conference on Computer Vision and Pattern Recognition